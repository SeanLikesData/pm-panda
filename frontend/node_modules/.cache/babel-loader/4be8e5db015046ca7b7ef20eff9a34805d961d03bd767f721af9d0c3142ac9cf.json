{"ast":null,"code":"import axios from 'axios';\nclass OllamaService {\n  constructor() {\n    this.baseURL = 'http://localhost:11434';\n    this.model = 'mistral:7b-instruct'; // Using your Mistral model\n  }\n  async checkConnection() {\n    try {\n      const response = await axios.get(`${this.baseURL}/api/tags`);\n      return response.status === 200;\n    } catch (error) {\n      console.error('Ollama connection check failed:', error);\n      return false;\n    }\n  }\n  async getModels() {\n    try {\n      const response = await axios.get(`${this.baseURL}/api/tags`);\n      return response.data.models || [];\n    } catch (error) {\n      console.error('Failed to get Ollama models:', error);\n      return [];\n    }\n  }\n  async chat(prompt, context = '') {\n    try {\n      console.log('Sending request to Ollama:', this.model);\n      const response = await axios.post(`${this.baseURL}/api/generate`, {\n        model: this.model,\n        prompt: prompt,\n        stream: false\n      }, {\n        headers: {\n          'Content-Type': 'application/json'\n        }\n      });\n      console.log('Ollama response received');\n      return response.data.response || response.data;\n    } catch (error) {\n      var _error$response, _error$response2, _error$response2$data;\n      console.error('Ollama chat error details:', ((_error$response = error.response) === null || _error$response === void 0 ? void 0 : _error$response.data) || error.message);\n\n      // Try with qwen model as fallback\n      if (this.model !== 'qwen:32b') {\n        console.log('Trying with qwen:32b model...');\n        this.model = 'qwen:32b';\n        return this.chat(prompt, context);\n      }\n      throw new Error(`Ollama error: ${((_error$response2 = error.response) === null || _error$response2 === void 0 ? void 0 : (_error$response2$data = _error$response2.data) === null || _error$response2$data === void 0 ? void 0 : _error$response2$data.error) || error.message || 'Connection failed'}`);\n    }\n  }\n  async streamChat(prompt, context = '', onChunk) {\n    try {\n      const response = await fetch(`${this.baseURL}/api/generate`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({\n          model: this.model,\n          prompt: prompt,\n          context: context,\n          stream: true\n        })\n      });\n      const reader = response.body.getReader();\n      const decoder = new TextDecoder();\n      let fullResponse = '';\n      while (true) {\n        const {\n          done,\n          value\n        } = await reader.read();\n        if (done) break;\n        const chunk = decoder.decode(value);\n        const lines = chunk.split('\\n').filter(line => line.trim() !== '');\n        for (const line of lines) {\n          try {\n            const json = JSON.parse(line);\n            if (json.response) {\n              fullResponse += json.response;\n              onChunk(json.response);\n            }\n          } catch (e) {\n            console.error('Failed to parse chunk:', e);\n          }\n        }\n      }\n      return fullResponse;\n    } catch (error) {\n      console.error('Ollama stream chat error:', error);\n      throw new Error('Failed to stream response from Ollama');\n    }\n  }\n  setModel(modelName) {\n    this.model = modelName;\n  }\n  async generatePRDSuggestions(prdSection, currentContent) {\n    const prompt = `\n      As a Product Manager, provide specific suggestions to improve this section of a Product Requirements Document.\n      \n      Section: ${prdSection.title}\n      Current Content: ${currentContent || 'Empty'}\n      \n      Provide 3-5 specific, actionable suggestions to improve this section.\n      Format your response as a numbered list.\n    `;\n    return await this.chat(prompt);\n  }\n  async analyzePRD(prd) {\n    const prompt = `\n      Analyze this Product Requirements Document and provide feedback.\n      \n      PRD: ${JSON.stringify(prd, null, 2)}\n      \n      Provide:\n      1. Completeness score (0-100%)\n      2. Missing critical sections\n      3. Areas that need more detail\n      4. Strengths of the current PRD\n      5. Top 3 recommendations for improvement\n      \n      Format your response in a clear, structured way.\n    `;\n    return await this.chat(prompt);\n  }\n}\nexport const ollamaService = new OllamaService();","map":{"version":3,"names":["axios","OllamaService","constructor","baseURL","model","checkConnection","response","get","status","error","console","getModels","data","models","chat","prompt","context","log","post","stream","headers","_error$response","_error$response2","_error$response2$data","message","Error","streamChat","onChunk","fetch","method","body","JSON","stringify","reader","getReader","decoder","TextDecoder","fullResponse","done","value","read","chunk","decode","lines","split","filter","line","trim","json","parse","e","setModel","modelName","generatePRDSuggestions","prdSection","currentContent","title","analyzePRD","prd","ollamaService"],"sources":["/Volumes/t9/github/pmhelper/frontend/src/services/ollamaService.js"],"sourcesContent":["import axios from 'axios';\n\nclass OllamaService {\n  constructor() {\n    this.baseURL = 'http://localhost:11434';\n    this.model = 'mistral:7b-instruct'; // Using your Mistral model\n  }\n\n  async checkConnection() {\n    try {\n      const response = await axios.get(`${this.baseURL}/api/tags`);\n      return response.status === 200;\n    } catch (error) {\n      console.error('Ollama connection check failed:', error);\n      return false;\n    }\n  }\n\n  async getModels() {\n    try {\n      const response = await axios.get(`${this.baseURL}/api/tags`);\n      return response.data.models || [];\n    } catch (error) {\n      console.error('Failed to get Ollama models:', error);\n      return [];\n    }\n  }\n\n  async chat(prompt, context = '') {\n    try {\n      console.log('Sending request to Ollama:', this.model);\n      const response = await axios.post(`${this.baseURL}/api/generate`, {\n        model: this.model,\n        prompt: prompt,\n        stream: false\n      }, {\n        headers: {\n          'Content-Type': 'application/json'\n        }\n      });\n\n      console.log('Ollama response received');\n      return response.data.response || response.data;\n    } catch (error) {\n      console.error('Ollama chat error details:', error.response?.data || error.message);\n      \n      // Try with qwen model as fallback\n      if (this.model !== 'qwen:32b') {\n        console.log('Trying with qwen:32b model...');\n        this.model = 'qwen:32b';\n        return this.chat(prompt, context);\n      }\n      \n      throw new Error(`Ollama error: ${error.response?.data?.error || error.message || 'Connection failed'}`);\n    }\n  }\n\n  async streamChat(prompt, context = '', onChunk) {\n    try {\n      const response = await fetch(`${this.baseURL}/api/generate`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({\n          model: this.model,\n          prompt: prompt,\n          context: context,\n          stream: true\n        })\n      });\n\n      const reader = response.body.getReader();\n      const decoder = new TextDecoder();\n      let fullResponse = '';\n\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        const chunk = decoder.decode(value);\n        const lines = chunk.split('\\n').filter(line => line.trim() !== '');\n        \n        for (const line of lines) {\n          try {\n            const json = JSON.parse(line);\n            if (json.response) {\n              fullResponse += json.response;\n              onChunk(json.response);\n            }\n          } catch (e) {\n            console.error('Failed to parse chunk:', e);\n          }\n        }\n      }\n\n      return fullResponse;\n    } catch (error) {\n      console.error('Ollama stream chat error:', error);\n      throw new Error('Failed to stream response from Ollama');\n    }\n  }\n\n  setModel(modelName) {\n    this.model = modelName;\n  }\n\n  async generatePRDSuggestions(prdSection, currentContent) {\n    const prompt = `\n      As a Product Manager, provide specific suggestions to improve this section of a Product Requirements Document.\n      \n      Section: ${prdSection.title}\n      Current Content: ${currentContent || 'Empty'}\n      \n      Provide 3-5 specific, actionable suggestions to improve this section.\n      Format your response as a numbered list.\n    `;\n\n    return await this.chat(prompt);\n  }\n\n  async analyzePRD(prd) {\n    const prompt = `\n      Analyze this Product Requirements Document and provide feedback.\n      \n      PRD: ${JSON.stringify(prd, null, 2)}\n      \n      Provide:\n      1. Completeness score (0-100%)\n      2. Missing critical sections\n      3. Areas that need more detail\n      4. Strengths of the current PRD\n      5. Top 3 recommendations for improvement\n      \n      Format your response in a clear, structured way.\n    `;\n\n    return await this.chat(prompt);\n  }\n}\n\nexport const ollamaService = new OllamaService();"],"mappings":"AAAA,OAAOA,KAAK,MAAM,OAAO;AAEzB,MAAMC,aAAa,CAAC;EAClBC,WAAWA,CAAA,EAAG;IACZ,IAAI,CAACC,OAAO,GAAG,wBAAwB;IACvC,IAAI,CAACC,KAAK,GAAG,qBAAqB,CAAC,CAAC;EACtC;EAEA,MAAMC,eAAeA,CAAA,EAAG;IACtB,IAAI;MACF,MAAMC,QAAQ,GAAG,MAAMN,KAAK,CAACO,GAAG,CAAC,GAAG,IAAI,CAACJ,OAAO,WAAW,CAAC;MAC5D,OAAOG,QAAQ,CAACE,MAAM,KAAK,GAAG;IAChC,CAAC,CAAC,OAAOC,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,iCAAiC,EAAEA,KAAK,CAAC;MACvD,OAAO,KAAK;IACd;EACF;EAEA,MAAME,SAASA,CAAA,EAAG;IAChB,IAAI;MACF,MAAML,QAAQ,GAAG,MAAMN,KAAK,CAACO,GAAG,CAAC,GAAG,IAAI,CAACJ,OAAO,WAAW,CAAC;MAC5D,OAAOG,QAAQ,CAACM,IAAI,CAACC,MAAM,IAAI,EAAE;IACnC,CAAC,CAAC,OAAOJ,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,8BAA8B,EAAEA,KAAK,CAAC;MACpD,OAAO,EAAE;IACX;EACF;EAEA,MAAMK,IAAIA,CAACC,MAAM,EAAEC,OAAO,GAAG,EAAE,EAAE;IAC/B,IAAI;MACFN,OAAO,CAACO,GAAG,CAAC,4BAA4B,EAAE,IAAI,CAACb,KAAK,CAAC;MACrD,MAAME,QAAQ,GAAG,MAAMN,KAAK,CAACkB,IAAI,CAAC,GAAG,IAAI,CAACf,OAAO,eAAe,EAAE;QAChEC,KAAK,EAAE,IAAI,CAACA,KAAK;QACjBW,MAAM,EAAEA,MAAM;QACdI,MAAM,EAAE;MACV,CAAC,EAAE;QACDC,OAAO,EAAE;UACP,cAAc,EAAE;QAClB;MACF,CAAC,CAAC;MAEFV,OAAO,CAACO,GAAG,CAAC,0BAA0B,CAAC;MACvC,OAAOX,QAAQ,CAACM,IAAI,CAACN,QAAQ,IAAIA,QAAQ,CAACM,IAAI;IAChD,CAAC,CAAC,OAAOH,KAAK,EAAE;MAAA,IAAAY,eAAA,EAAAC,gBAAA,EAAAC,qBAAA;MACdb,OAAO,CAACD,KAAK,CAAC,4BAA4B,EAAE,EAAAY,eAAA,GAAAZ,KAAK,CAACH,QAAQ,cAAAe,eAAA,uBAAdA,eAAA,CAAgBT,IAAI,KAAIH,KAAK,CAACe,OAAO,CAAC;;MAElF;MACA,IAAI,IAAI,CAACpB,KAAK,KAAK,UAAU,EAAE;QAC7BM,OAAO,CAACO,GAAG,CAAC,+BAA+B,CAAC;QAC5C,IAAI,CAACb,KAAK,GAAG,UAAU;QACvB,OAAO,IAAI,CAACU,IAAI,CAACC,MAAM,EAAEC,OAAO,CAAC;MACnC;MAEA,MAAM,IAAIS,KAAK,CAAC,iBAAiB,EAAAH,gBAAA,GAAAb,KAAK,CAACH,QAAQ,cAAAgB,gBAAA,wBAAAC,qBAAA,GAAdD,gBAAA,CAAgBV,IAAI,cAAAW,qBAAA,uBAApBA,qBAAA,CAAsBd,KAAK,KAAIA,KAAK,CAACe,OAAO,IAAI,mBAAmB,EAAE,CAAC;IACzG;EACF;EAEA,MAAME,UAAUA,CAACX,MAAM,EAAEC,OAAO,GAAG,EAAE,EAAEW,OAAO,EAAE;IAC9C,IAAI;MACF,MAAMrB,QAAQ,GAAG,MAAMsB,KAAK,CAAC,GAAG,IAAI,CAACzB,OAAO,eAAe,EAAE;QAC3D0B,MAAM,EAAE,MAAM;QACdT,OAAO,EAAE;UACP,cAAc,EAAE;QAClB,CAAC;QACDU,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;UACnB5B,KAAK,EAAE,IAAI,CAACA,KAAK;UACjBW,MAAM,EAAEA,MAAM;UACdC,OAAO,EAAEA,OAAO;UAChBG,MAAM,EAAE;QACV,CAAC;MACH,CAAC,CAAC;MAEF,MAAMc,MAAM,GAAG3B,QAAQ,CAACwB,IAAI,CAACI,SAAS,CAAC,CAAC;MACxC,MAAMC,OAAO,GAAG,IAAIC,WAAW,CAAC,CAAC;MACjC,IAAIC,YAAY,GAAG,EAAE;MAErB,OAAO,IAAI,EAAE;QACX,MAAM;UAAEC,IAAI;UAAEC;QAAM,CAAC,GAAG,MAAMN,MAAM,CAACO,IAAI,CAAC,CAAC;QAC3C,IAAIF,IAAI,EAAE;QAEV,MAAMG,KAAK,GAAGN,OAAO,CAACO,MAAM,CAACH,KAAK,CAAC;QACnC,MAAMI,KAAK,GAAGF,KAAK,CAACG,KAAK,CAAC,IAAI,CAAC,CAACC,MAAM,CAACC,IAAI,IAAIA,IAAI,CAACC,IAAI,CAAC,CAAC,KAAK,EAAE,CAAC;QAElE,KAAK,MAAMD,IAAI,IAAIH,KAAK,EAAE;UACxB,IAAI;YACF,MAAMK,IAAI,GAAGjB,IAAI,CAACkB,KAAK,CAACH,IAAI,CAAC;YAC7B,IAAIE,IAAI,CAAC1C,QAAQ,EAAE;cACjB+B,YAAY,IAAIW,IAAI,CAAC1C,QAAQ;cAC7BqB,OAAO,CAACqB,IAAI,CAAC1C,QAAQ,CAAC;YACxB;UACF,CAAC,CAAC,OAAO4C,CAAC,EAAE;YACVxC,OAAO,CAACD,KAAK,CAAC,wBAAwB,EAAEyC,CAAC,CAAC;UAC5C;QACF;MACF;MAEA,OAAOb,YAAY;IACrB,CAAC,CAAC,OAAO5B,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,2BAA2B,EAAEA,KAAK,CAAC;MACjD,MAAM,IAAIgB,KAAK,CAAC,uCAAuC,CAAC;IAC1D;EACF;EAEA0B,QAAQA,CAACC,SAAS,EAAE;IAClB,IAAI,CAAChD,KAAK,GAAGgD,SAAS;EACxB;EAEA,MAAMC,sBAAsBA,CAACC,UAAU,EAAEC,cAAc,EAAE;IACvD,MAAMxC,MAAM,GAAG;AACnB;AACA;AACA,iBAAiBuC,UAAU,CAACE,KAAK;AACjC,yBAAyBD,cAAc,IAAI,OAAO;AAClD;AACA;AACA;AACA,KAAK;IAED,OAAO,MAAM,IAAI,CAACzC,IAAI,CAACC,MAAM,CAAC;EAChC;EAEA,MAAM0C,UAAUA,CAACC,GAAG,EAAE;IACpB,MAAM3C,MAAM,GAAG;AACnB;AACA;AACA,aAAagB,IAAI,CAACC,SAAS,CAAC0B,GAAG,EAAE,IAAI,EAAE,CAAC,CAAC;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;IAED,OAAO,MAAM,IAAI,CAAC5C,IAAI,CAACC,MAAM,CAAC;EAChC;AACF;AAEA,OAAO,MAAM4C,aAAa,GAAG,IAAI1D,aAAa,CAAC,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}