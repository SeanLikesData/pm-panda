{"ast":null,"code":"import axios from 'axios';\nclass OllamaService {\n  constructor() {\n    this.baseURL = 'http://localhost:11434';\n    this.model = 'mistral:7b-instruct'; // Using your Mistral model\n  }\n  async checkConnection() {\n    try {\n      const response = await axios.get(`${this.baseURL}/api/tags`);\n      return response.status === 200;\n    } catch (error) {\n      console.error('Ollama connection check failed:', error);\n      return false;\n    }\n  }\n  async getModels() {\n    try {\n      const response = await axios.get(`${this.baseURL}/api/tags`);\n      return response.data.models || [];\n    } catch (error) {\n      console.error('Failed to get Ollama models:', error);\n      return [];\n    }\n  }\n  async chat(prompt, context = '') {\n    try {\n      var _response$data$respon;\n      // Enhanced logging\n      const logData = {\n        timestamp: new Date().toISOString(),\n        model: this.model,\n        promptLength: prompt.length,\n        contextLength: context.length,\n        prompt: prompt,\n        context: context\n      };\n      console.log('===== OLLAMA REQUEST START =====');\n      console.log('Timestamp:', logData.timestamp);\n      console.log('Model:', logData.model);\n      console.log('Prompt Length:', logData.promptLength);\n      console.log('Context Length:', logData.contextLength);\n      console.log('Full Prompt:', prompt);\n      if (context) {\n        console.log('Context:', context);\n      }\n      console.log('===== OLLAMA REQUEST END =====');\n\n      // Add timeout and simplified prompt\n      const controller = new AbortController();\n      const timeoutId = setTimeout(() => controller.abort(), 15000); // 15 second timeout\n\n      const response = await axios.post(`${this.baseURL}/api/generate`, {\n        model: this.model,\n        prompt: prompt,\n        stream: false,\n        context: context,\n        options: {\n          temperature: 0.7,\n          top_k: 40,\n          top_p: 0.9,\n          num_predict: 500 // Increased for better responses\n        }\n      }, {\n        headers: {\n          'Content-Type': 'application/json'\n        },\n        signal: controller.signal,\n        timeout: 15000\n      });\n      clearTimeout(timeoutId);\n      console.log('===== OLLAMA RESPONSE START =====');\n      console.log('Response received at:', new Date().toISOString());\n      console.log('Response length:', ((_response$data$respon = response.data.response) === null || _response$data$respon === void 0 ? void 0 : _response$data$respon.length) || 0);\n      console.log('Response:', response.data.response || response.data);\n      console.log('===== OLLAMA RESPONSE END =====');\n      return response.data.response || response.data;\n    } catch (error) {\n      console.error('===== OLLAMA ERROR =====');\n      console.error('Error at:', new Date().toISOString());\n      console.error('Error message:', error.message);\n      console.error('Error details:', error);\n      if (error.code === 'ECONNABORTED' || error.message.includes('timeout')) {\n        throw new Error('Request timeout - Ollama is taking too long to respond');\n      }\n      throw new Error('Ollama is not responding. Using fallback responses.');\n    }\n  }\n  async streamChat(prompt, context = '', onChunk) {\n    try {\n      const response = await fetch(`${this.baseURL}/api/generate`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({\n          model: this.model,\n          prompt: prompt,\n          context: context,\n          stream: true\n        })\n      });\n      const reader = response.body.getReader();\n      const decoder = new TextDecoder();\n      let fullResponse = '';\n      while (true) {\n        const {\n          done,\n          value\n        } = await reader.read();\n        if (done) break;\n        const chunk = decoder.decode(value);\n        const lines = chunk.split('\\n').filter(line => line.trim() !== '');\n        for (const line of lines) {\n          try {\n            const json = JSON.parse(line);\n            if (json.response) {\n              fullResponse += json.response;\n              onChunk(json.response);\n            }\n          } catch (e) {\n            console.error('Failed to parse chunk:', e);\n          }\n        }\n      }\n      return fullResponse;\n    } catch (error) {\n      console.error('Ollama stream chat error:', error);\n      throw new Error('Failed to stream response from Ollama');\n    }\n  }\n  setModel(modelName) {\n    this.model = modelName;\n  }\n  async generatePRDSuggestions(prdSection, currentContent) {\n    const prompt = `\n      As a Product Manager, provide specific suggestions to improve this section of a Product Requirements Document.\n      \n      Section: ${prdSection.title}\n      Current Content: ${currentContent || 'Empty'}\n      \n      Provide 3-5 specific, actionable suggestions to improve this section.\n      Format your response as a numbered list.\n    `;\n    return await this.chat(prompt);\n  }\n  async analyzePRD(prd) {\n    const prompt = `\n      Analyze this Product Requirements Document and provide feedback.\n      \n      PRD: ${JSON.stringify(prd, null, 2)}\n      \n      Provide:\n      1. Completeness score (0-100%)\n      2. Missing critical sections\n      3. Areas that need more detail\n      4. Strengths of the current PRD\n      5. Top 3 recommendations for improvement\n      \n      Format your response in a clear, structured way.\n    `;\n    return await this.chat(prompt);\n  }\n}\nexport const ollamaService = new OllamaService();","map":{"version":3,"names":["axios","OllamaService","constructor","baseURL","model","checkConnection","response","get","status","error","console","getModels","data","models","chat","prompt","context","_response$data$respon","logData","timestamp","Date","toISOString","promptLength","length","contextLength","log","controller","AbortController","timeoutId","setTimeout","abort","post","stream","options","temperature","top_k","top_p","num_predict","headers","signal","timeout","clearTimeout","message","code","includes","Error","streamChat","onChunk","fetch","method","body","JSON","stringify","reader","getReader","decoder","TextDecoder","fullResponse","done","value","read","chunk","decode","lines","split","filter","line","trim","json","parse","e","setModel","modelName","generatePRDSuggestions","prdSection","currentContent","title","analyzePRD","prd","ollamaService"],"sources":["/Volumes/t9/github/pmhelper/frontend/src/services/ollamaService.js"],"sourcesContent":["import axios from 'axios';\n\nclass OllamaService {\n  constructor() {\n    this.baseURL = 'http://localhost:11434';\n    this.model = 'mistral:7b-instruct'; // Using your Mistral model\n  }\n\n  async checkConnection() {\n    try {\n      const response = await axios.get(`${this.baseURL}/api/tags`);\n      return response.status === 200;\n    } catch (error) {\n      console.error('Ollama connection check failed:', error);\n      return false;\n    }\n  }\n\n  async getModels() {\n    try {\n      const response = await axios.get(`${this.baseURL}/api/tags`);\n      return response.data.models || [];\n    } catch (error) {\n      console.error('Failed to get Ollama models:', error);\n      return [];\n    }\n  }\n\n  async chat(prompt, context = '') {\n    try {\n      // Enhanced logging\n      const logData = {\n        timestamp: new Date().toISOString(),\n        model: this.model,\n        promptLength: prompt.length,\n        contextLength: context.length,\n        prompt: prompt,\n        context: context\n      };\n      \n      console.log('===== OLLAMA REQUEST START =====');\n      console.log('Timestamp:', logData.timestamp);\n      console.log('Model:', logData.model);\n      console.log('Prompt Length:', logData.promptLength);\n      console.log('Context Length:', logData.contextLength);\n      console.log('Full Prompt:', prompt);\n      if (context) {\n        console.log('Context:', context);\n      }\n      console.log('===== OLLAMA REQUEST END =====');\n      \n      // Add timeout and simplified prompt\n      const controller = new AbortController();\n      const timeoutId = setTimeout(() => controller.abort(), 15000); // 15 second timeout\n      \n      const response = await axios.post(`${this.baseURL}/api/generate`, {\n        model: this.model,\n        prompt: prompt,\n        stream: false,\n        context: context,\n        options: {\n          temperature: 0.7,\n          top_k: 40,\n          top_p: 0.9,\n          num_predict: 500 // Increased for better responses\n        }\n      }, {\n        headers: {\n          'Content-Type': 'application/json'\n        },\n        signal: controller.signal,\n        timeout: 15000\n      });\n\n      clearTimeout(timeoutId);\n      \n      console.log('===== OLLAMA RESPONSE START =====');\n      console.log('Response received at:', new Date().toISOString());\n      console.log('Response length:', response.data.response?.length || 0);\n      console.log('Response:', response.data.response || response.data);\n      console.log('===== OLLAMA RESPONSE END =====');\n      \n      return response.data.response || response.data;\n    } catch (error) {\n      console.error('===== OLLAMA ERROR =====');\n      console.error('Error at:', new Date().toISOString());\n      console.error('Error message:', error.message);\n      console.error('Error details:', error);\n      \n      if (error.code === 'ECONNABORTED' || error.message.includes('timeout')) {\n        throw new Error('Request timeout - Ollama is taking too long to respond');\n      }\n      \n      throw new Error('Ollama is not responding. Using fallback responses.');\n    }\n  }\n\n  async streamChat(prompt, context = '', onChunk) {\n    try {\n      const response = await fetch(`${this.baseURL}/api/generate`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({\n          model: this.model,\n          prompt: prompt,\n          context: context,\n          stream: true\n        })\n      });\n\n      const reader = response.body.getReader();\n      const decoder = new TextDecoder();\n      let fullResponse = '';\n\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        const chunk = decoder.decode(value);\n        const lines = chunk.split('\\n').filter(line => line.trim() !== '');\n        \n        for (const line of lines) {\n          try {\n            const json = JSON.parse(line);\n            if (json.response) {\n              fullResponse += json.response;\n              onChunk(json.response);\n            }\n          } catch (e) {\n            console.error('Failed to parse chunk:', e);\n          }\n        }\n      }\n\n      return fullResponse;\n    } catch (error) {\n      console.error('Ollama stream chat error:', error);\n      throw new Error('Failed to stream response from Ollama');\n    }\n  }\n\n  setModel(modelName) {\n    this.model = modelName;\n  }\n\n  async generatePRDSuggestions(prdSection, currentContent) {\n    const prompt = `\n      As a Product Manager, provide specific suggestions to improve this section of a Product Requirements Document.\n      \n      Section: ${prdSection.title}\n      Current Content: ${currentContent || 'Empty'}\n      \n      Provide 3-5 specific, actionable suggestions to improve this section.\n      Format your response as a numbered list.\n    `;\n\n    return await this.chat(prompt);\n  }\n\n  async analyzePRD(prd) {\n    const prompt = `\n      Analyze this Product Requirements Document and provide feedback.\n      \n      PRD: ${JSON.stringify(prd, null, 2)}\n      \n      Provide:\n      1. Completeness score (0-100%)\n      2. Missing critical sections\n      3. Areas that need more detail\n      4. Strengths of the current PRD\n      5. Top 3 recommendations for improvement\n      \n      Format your response in a clear, structured way.\n    `;\n\n    return await this.chat(prompt);\n  }\n}\n\nexport const ollamaService = new OllamaService();"],"mappings":"AAAA,OAAOA,KAAK,MAAM,OAAO;AAEzB,MAAMC,aAAa,CAAC;EAClBC,WAAWA,CAAA,EAAG;IACZ,IAAI,CAACC,OAAO,GAAG,wBAAwB;IACvC,IAAI,CAACC,KAAK,GAAG,qBAAqB,CAAC,CAAC;EACtC;EAEA,MAAMC,eAAeA,CAAA,EAAG;IACtB,IAAI;MACF,MAAMC,QAAQ,GAAG,MAAMN,KAAK,CAACO,GAAG,CAAC,GAAG,IAAI,CAACJ,OAAO,WAAW,CAAC;MAC5D,OAAOG,QAAQ,CAACE,MAAM,KAAK,GAAG;IAChC,CAAC,CAAC,OAAOC,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,iCAAiC,EAAEA,KAAK,CAAC;MACvD,OAAO,KAAK;IACd;EACF;EAEA,MAAME,SAASA,CAAA,EAAG;IAChB,IAAI;MACF,MAAML,QAAQ,GAAG,MAAMN,KAAK,CAACO,GAAG,CAAC,GAAG,IAAI,CAACJ,OAAO,WAAW,CAAC;MAC5D,OAAOG,QAAQ,CAACM,IAAI,CAACC,MAAM,IAAI,EAAE;IACnC,CAAC,CAAC,OAAOJ,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,8BAA8B,EAAEA,KAAK,CAAC;MACpD,OAAO,EAAE;IACX;EACF;EAEA,MAAMK,IAAIA,CAACC,MAAM,EAAEC,OAAO,GAAG,EAAE,EAAE;IAC/B,IAAI;MAAA,IAAAC,qBAAA;MACF;MACA,MAAMC,OAAO,GAAG;QACdC,SAAS,EAAE,IAAIC,IAAI,CAAC,CAAC,CAACC,WAAW,CAAC,CAAC;QACnCjB,KAAK,EAAE,IAAI,CAACA,KAAK;QACjBkB,YAAY,EAAEP,MAAM,CAACQ,MAAM;QAC3BC,aAAa,EAAER,OAAO,CAACO,MAAM;QAC7BR,MAAM,EAAEA,MAAM;QACdC,OAAO,EAAEA;MACX,CAAC;MAEDN,OAAO,CAACe,GAAG,CAAC,kCAAkC,CAAC;MAC/Cf,OAAO,CAACe,GAAG,CAAC,YAAY,EAAEP,OAAO,CAACC,SAAS,CAAC;MAC5CT,OAAO,CAACe,GAAG,CAAC,QAAQ,EAAEP,OAAO,CAACd,KAAK,CAAC;MACpCM,OAAO,CAACe,GAAG,CAAC,gBAAgB,EAAEP,OAAO,CAACI,YAAY,CAAC;MACnDZ,OAAO,CAACe,GAAG,CAAC,iBAAiB,EAAEP,OAAO,CAACM,aAAa,CAAC;MACrDd,OAAO,CAACe,GAAG,CAAC,cAAc,EAAEV,MAAM,CAAC;MACnC,IAAIC,OAAO,EAAE;QACXN,OAAO,CAACe,GAAG,CAAC,UAAU,EAAET,OAAO,CAAC;MAClC;MACAN,OAAO,CAACe,GAAG,CAAC,gCAAgC,CAAC;;MAE7C;MACA,MAAMC,UAAU,GAAG,IAAIC,eAAe,CAAC,CAAC;MACxC,MAAMC,SAAS,GAAGC,UAAU,CAAC,MAAMH,UAAU,CAACI,KAAK,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC,CAAC;;MAE/D,MAAMxB,QAAQ,GAAG,MAAMN,KAAK,CAAC+B,IAAI,CAAC,GAAG,IAAI,CAAC5B,OAAO,eAAe,EAAE;QAChEC,KAAK,EAAE,IAAI,CAACA,KAAK;QACjBW,MAAM,EAAEA,MAAM;QACdiB,MAAM,EAAE,KAAK;QACbhB,OAAO,EAAEA,OAAO;QAChBiB,OAAO,EAAE;UACPC,WAAW,EAAE,GAAG;UAChBC,KAAK,EAAE,EAAE;UACTC,KAAK,EAAE,GAAG;UACVC,WAAW,EAAE,GAAG,CAAC;QACnB;MACF,CAAC,EAAE;QACDC,OAAO,EAAE;UACP,cAAc,EAAE;QAClB,CAAC;QACDC,MAAM,EAAEb,UAAU,CAACa,MAAM;QACzBC,OAAO,EAAE;MACX,CAAC,CAAC;MAEFC,YAAY,CAACb,SAAS,CAAC;MAEvBlB,OAAO,CAACe,GAAG,CAAC,mCAAmC,CAAC;MAChDf,OAAO,CAACe,GAAG,CAAC,uBAAuB,EAAE,IAAIL,IAAI,CAAC,CAAC,CAACC,WAAW,CAAC,CAAC,CAAC;MAC9DX,OAAO,CAACe,GAAG,CAAC,kBAAkB,EAAE,EAAAR,qBAAA,GAAAX,QAAQ,CAACM,IAAI,CAACN,QAAQ,cAAAW,qBAAA,uBAAtBA,qBAAA,CAAwBM,MAAM,KAAI,CAAC,CAAC;MACpEb,OAAO,CAACe,GAAG,CAAC,WAAW,EAAEnB,QAAQ,CAACM,IAAI,CAACN,QAAQ,IAAIA,QAAQ,CAACM,IAAI,CAAC;MACjEF,OAAO,CAACe,GAAG,CAAC,iCAAiC,CAAC;MAE9C,OAAOnB,QAAQ,CAACM,IAAI,CAACN,QAAQ,IAAIA,QAAQ,CAACM,IAAI;IAChD,CAAC,CAAC,OAAOH,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,0BAA0B,CAAC;MACzCC,OAAO,CAACD,KAAK,CAAC,WAAW,EAAE,IAAIW,IAAI,CAAC,CAAC,CAACC,WAAW,CAAC,CAAC,CAAC;MACpDX,OAAO,CAACD,KAAK,CAAC,gBAAgB,EAAEA,KAAK,CAACiC,OAAO,CAAC;MAC9ChC,OAAO,CAACD,KAAK,CAAC,gBAAgB,EAAEA,KAAK,CAAC;MAEtC,IAAIA,KAAK,CAACkC,IAAI,KAAK,cAAc,IAAIlC,KAAK,CAACiC,OAAO,CAACE,QAAQ,CAAC,SAAS,CAAC,EAAE;QACtE,MAAM,IAAIC,KAAK,CAAC,wDAAwD,CAAC;MAC3E;MAEA,MAAM,IAAIA,KAAK,CAAC,qDAAqD,CAAC;IACxE;EACF;EAEA,MAAMC,UAAUA,CAAC/B,MAAM,EAAEC,OAAO,GAAG,EAAE,EAAE+B,OAAO,EAAE;IAC9C,IAAI;MACF,MAAMzC,QAAQ,GAAG,MAAM0C,KAAK,CAAC,GAAG,IAAI,CAAC7C,OAAO,eAAe,EAAE;QAC3D8C,MAAM,EAAE,MAAM;QACdX,OAAO,EAAE;UACP,cAAc,EAAE;QAClB,CAAC;QACDY,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;UACnBhD,KAAK,EAAE,IAAI,CAACA,KAAK;UACjBW,MAAM,EAAEA,MAAM;UACdC,OAAO,EAAEA,OAAO;UAChBgB,MAAM,EAAE;QACV,CAAC;MACH,CAAC,CAAC;MAEF,MAAMqB,MAAM,GAAG/C,QAAQ,CAAC4C,IAAI,CAACI,SAAS,CAAC,CAAC;MACxC,MAAMC,OAAO,GAAG,IAAIC,WAAW,CAAC,CAAC;MACjC,IAAIC,YAAY,GAAG,EAAE;MAErB,OAAO,IAAI,EAAE;QACX,MAAM;UAAEC,IAAI;UAAEC;QAAM,CAAC,GAAG,MAAMN,MAAM,CAACO,IAAI,CAAC,CAAC;QAC3C,IAAIF,IAAI,EAAE;QAEV,MAAMG,KAAK,GAAGN,OAAO,CAACO,MAAM,CAACH,KAAK,CAAC;QACnC,MAAMI,KAAK,GAAGF,KAAK,CAACG,KAAK,CAAC,IAAI,CAAC,CAACC,MAAM,CAACC,IAAI,IAAIA,IAAI,CAACC,IAAI,CAAC,CAAC,KAAK,EAAE,CAAC;QAElE,KAAK,MAAMD,IAAI,IAAIH,KAAK,EAAE;UACxB,IAAI;YACF,MAAMK,IAAI,GAAGjB,IAAI,CAACkB,KAAK,CAACH,IAAI,CAAC;YAC7B,IAAIE,IAAI,CAAC9D,QAAQ,EAAE;cACjBmD,YAAY,IAAIW,IAAI,CAAC9D,QAAQ;cAC7ByC,OAAO,CAACqB,IAAI,CAAC9D,QAAQ,CAAC;YACxB;UACF,CAAC,CAAC,OAAOgE,CAAC,EAAE;YACV5D,OAAO,CAACD,KAAK,CAAC,wBAAwB,EAAE6D,CAAC,CAAC;UAC5C;QACF;MACF;MAEA,OAAOb,YAAY;IACrB,CAAC,CAAC,OAAOhD,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,2BAA2B,EAAEA,KAAK,CAAC;MACjD,MAAM,IAAIoC,KAAK,CAAC,uCAAuC,CAAC;IAC1D;EACF;EAEA0B,QAAQA,CAACC,SAAS,EAAE;IAClB,IAAI,CAACpE,KAAK,GAAGoE,SAAS;EACxB;EAEA,MAAMC,sBAAsBA,CAACC,UAAU,EAAEC,cAAc,EAAE;IACvD,MAAM5D,MAAM,GAAG;AACnB;AACA;AACA,iBAAiB2D,UAAU,CAACE,KAAK;AACjC,yBAAyBD,cAAc,IAAI,OAAO;AAClD;AACA;AACA;AACA,KAAK;IAED,OAAO,MAAM,IAAI,CAAC7D,IAAI,CAACC,MAAM,CAAC;EAChC;EAEA,MAAM8D,UAAUA,CAACC,GAAG,EAAE;IACpB,MAAM/D,MAAM,GAAG;AACnB;AACA;AACA,aAAaoC,IAAI,CAACC,SAAS,CAAC0B,GAAG,EAAE,IAAI,EAAE,CAAC,CAAC;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;IAED,OAAO,MAAM,IAAI,CAAChE,IAAI,CAACC,MAAM,CAAC;EAChC;AACF;AAEA,OAAO,MAAMgE,aAAa,GAAG,IAAI9E,aAAa,CAAC,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}