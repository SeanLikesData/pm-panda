{"ast":null,"code":"import axios from 'axios';\nclass OllamaService {\n  constructor() {\n    this.baseURL = 'http://localhost:11434';\n    this.model = 'mistral:7b-instruct'; // Using your Mistral model\n  }\n  async checkConnection() {\n    try {\n      const response = await axios.get(`${this.baseURL}/api/tags`);\n      return response.status === 200;\n    } catch (error) {\n      console.error('Ollama connection check failed:', error);\n      return false;\n    }\n  }\n  async getModels() {\n    try {\n      const response = await axios.get(`${this.baseURL}/api/tags`);\n      return response.data.models || [];\n    } catch (error) {\n      console.error('Failed to get Ollama models:', error);\n      return [];\n    }\n  }\n  async chat(prompt, context = '') {\n    try {\n      console.log('Sending request to Ollama:', this.model);\n\n      // Add timeout and simplified prompt\n      const controller = new AbortController();\n      const timeoutId = setTimeout(() => controller.abort(), 15000); // 15 second timeout\n\n      const response = await axios.post(`${this.baseURL}/api/generate`, {\n        model: this.model,\n        prompt: prompt.substring(0, 1000),\n        // Limit prompt length\n        stream: false,\n        options: {\n          temperature: 0.7,\n          top_k: 40,\n          top_p: 0.9,\n          num_predict: 200 // Limit response length for speed\n        }\n      }, {\n        headers: {\n          'Content-Type': 'application/json'\n        },\n        signal: controller.signal,\n        timeout: 15000\n      });\n      clearTimeout(timeoutId);\n      console.log('Ollama response received');\n      return response.data.response || response.data;\n    } catch (error) {\n      console.error('Ollama chat error:', error.message);\n      if (error.code === 'ECONNABORTED' || error.message.includes('timeout')) {\n        throw new Error('Request timeout - Ollama is taking too long to respond');\n      }\n      throw new Error('Ollama is not responding. Using fallback responses.');\n    }\n  }\n  async streamChat(prompt, context = '', onChunk) {\n    try {\n      const response = await fetch(`${this.baseURL}/api/generate`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({\n          model: this.model,\n          prompt: prompt,\n          context: context,\n          stream: true\n        })\n      });\n      const reader = response.body.getReader();\n      const decoder = new TextDecoder();\n      let fullResponse = '';\n      while (true) {\n        const {\n          done,\n          value\n        } = await reader.read();\n        if (done) break;\n        const chunk = decoder.decode(value);\n        const lines = chunk.split('\\n').filter(line => line.trim() !== '');\n        for (const line of lines) {\n          try {\n            const json = JSON.parse(line);\n            if (json.response) {\n              fullResponse += json.response;\n              onChunk(json.response);\n            }\n          } catch (e) {\n            console.error('Failed to parse chunk:', e);\n          }\n        }\n      }\n      return fullResponse;\n    } catch (error) {\n      console.error('Ollama stream chat error:', error);\n      throw new Error('Failed to stream response from Ollama');\n    }\n  }\n  setModel(modelName) {\n    this.model = modelName;\n  }\n  async generatePRDSuggestions(prdSection, currentContent) {\n    const prompt = `\n      As a Product Manager, provide specific suggestions to improve this section of a Product Requirements Document.\n      \n      Section: ${prdSection.title}\n      Current Content: ${currentContent || 'Empty'}\n      \n      Provide 3-5 specific, actionable suggestions to improve this section.\n      Format your response as a numbered list.\n    `;\n    return await this.chat(prompt);\n  }\n  async analyzePRD(prd) {\n    const prompt = `\n      Analyze this Product Requirements Document and provide feedback.\n      \n      PRD: ${JSON.stringify(prd, null, 2)}\n      \n      Provide:\n      1. Completeness score (0-100%)\n      2. Missing critical sections\n      3. Areas that need more detail\n      4. Strengths of the current PRD\n      5. Top 3 recommendations for improvement\n      \n      Format your response in a clear, structured way.\n    `;\n    return await this.chat(prompt);\n  }\n}\nexport const ollamaService = new OllamaService();","map":{"version":3,"names":["axios","OllamaService","constructor","baseURL","model","checkConnection","response","get","status","error","console","getModels","data","models","chat","prompt","context","log","controller","AbortController","timeoutId","setTimeout","abort","post","substring","stream","options","temperature","top_k","top_p","num_predict","headers","signal","timeout","clearTimeout","message","code","includes","Error","streamChat","onChunk","fetch","method","body","JSON","stringify","reader","getReader","decoder","TextDecoder","fullResponse","done","value","read","chunk","decode","lines","split","filter","line","trim","json","parse","e","setModel","modelName","generatePRDSuggestions","prdSection","currentContent","title","analyzePRD","prd","ollamaService"],"sources":["/Volumes/t9/github/pmhelper/frontend/src/services/ollamaService.js"],"sourcesContent":["import axios from 'axios';\n\nclass OllamaService {\n  constructor() {\n    this.baseURL = 'http://localhost:11434';\n    this.model = 'mistral:7b-instruct'; // Using your Mistral model\n  }\n\n  async checkConnection() {\n    try {\n      const response = await axios.get(`${this.baseURL}/api/tags`);\n      return response.status === 200;\n    } catch (error) {\n      console.error('Ollama connection check failed:', error);\n      return false;\n    }\n  }\n\n  async getModels() {\n    try {\n      const response = await axios.get(`${this.baseURL}/api/tags`);\n      return response.data.models || [];\n    } catch (error) {\n      console.error('Failed to get Ollama models:', error);\n      return [];\n    }\n  }\n\n  async chat(prompt, context = '') {\n    try {\n      console.log('Sending request to Ollama:', this.model);\n      \n      // Add timeout and simplified prompt\n      const controller = new AbortController();\n      const timeoutId = setTimeout(() => controller.abort(), 15000); // 15 second timeout\n      \n      const response = await axios.post(`${this.baseURL}/api/generate`, {\n        model: this.model,\n        prompt: prompt.substring(0, 1000), // Limit prompt length\n        stream: false,\n        options: {\n          temperature: 0.7,\n          top_k: 40,\n          top_p: 0.9,\n          num_predict: 200 // Limit response length for speed\n        }\n      }, {\n        headers: {\n          'Content-Type': 'application/json'\n        },\n        signal: controller.signal,\n        timeout: 15000\n      });\n\n      clearTimeout(timeoutId);\n      console.log('Ollama response received');\n      return response.data.response || response.data;\n    } catch (error) {\n      console.error('Ollama chat error:', error.message);\n      \n      if (error.code === 'ECONNABORTED' || error.message.includes('timeout')) {\n        throw new Error('Request timeout - Ollama is taking too long to respond');\n      }\n      \n      throw new Error('Ollama is not responding. Using fallback responses.');\n    }\n  }\n\n  async streamChat(prompt, context = '', onChunk) {\n    try {\n      const response = await fetch(`${this.baseURL}/api/generate`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({\n          model: this.model,\n          prompt: prompt,\n          context: context,\n          stream: true\n        })\n      });\n\n      const reader = response.body.getReader();\n      const decoder = new TextDecoder();\n      let fullResponse = '';\n\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        const chunk = decoder.decode(value);\n        const lines = chunk.split('\\n').filter(line => line.trim() !== '');\n        \n        for (const line of lines) {\n          try {\n            const json = JSON.parse(line);\n            if (json.response) {\n              fullResponse += json.response;\n              onChunk(json.response);\n            }\n          } catch (e) {\n            console.error('Failed to parse chunk:', e);\n          }\n        }\n      }\n\n      return fullResponse;\n    } catch (error) {\n      console.error('Ollama stream chat error:', error);\n      throw new Error('Failed to stream response from Ollama');\n    }\n  }\n\n  setModel(modelName) {\n    this.model = modelName;\n  }\n\n  async generatePRDSuggestions(prdSection, currentContent) {\n    const prompt = `\n      As a Product Manager, provide specific suggestions to improve this section of a Product Requirements Document.\n      \n      Section: ${prdSection.title}\n      Current Content: ${currentContent || 'Empty'}\n      \n      Provide 3-5 specific, actionable suggestions to improve this section.\n      Format your response as a numbered list.\n    `;\n\n    return await this.chat(prompt);\n  }\n\n  async analyzePRD(prd) {\n    const prompt = `\n      Analyze this Product Requirements Document and provide feedback.\n      \n      PRD: ${JSON.stringify(prd, null, 2)}\n      \n      Provide:\n      1. Completeness score (0-100%)\n      2. Missing critical sections\n      3. Areas that need more detail\n      4. Strengths of the current PRD\n      5. Top 3 recommendations for improvement\n      \n      Format your response in a clear, structured way.\n    `;\n\n    return await this.chat(prompt);\n  }\n}\n\nexport const ollamaService = new OllamaService();"],"mappings":"AAAA,OAAOA,KAAK,MAAM,OAAO;AAEzB,MAAMC,aAAa,CAAC;EAClBC,WAAWA,CAAA,EAAG;IACZ,IAAI,CAACC,OAAO,GAAG,wBAAwB;IACvC,IAAI,CAACC,KAAK,GAAG,qBAAqB,CAAC,CAAC;EACtC;EAEA,MAAMC,eAAeA,CAAA,EAAG;IACtB,IAAI;MACF,MAAMC,QAAQ,GAAG,MAAMN,KAAK,CAACO,GAAG,CAAC,GAAG,IAAI,CAACJ,OAAO,WAAW,CAAC;MAC5D,OAAOG,QAAQ,CAACE,MAAM,KAAK,GAAG;IAChC,CAAC,CAAC,OAAOC,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,iCAAiC,EAAEA,KAAK,CAAC;MACvD,OAAO,KAAK;IACd;EACF;EAEA,MAAME,SAASA,CAAA,EAAG;IAChB,IAAI;MACF,MAAML,QAAQ,GAAG,MAAMN,KAAK,CAACO,GAAG,CAAC,GAAG,IAAI,CAACJ,OAAO,WAAW,CAAC;MAC5D,OAAOG,QAAQ,CAACM,IAAI,CAACC,MAAM,IAAI,EAAE;IACnC,CAAC,CAAC,OAAOJ,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,8BAA8B,EAAEA,KAAK,CAAC;MACpD,OAAO,EAAE;IACX;EACF;EAEA,MAAMK,IAAIA,CAACC,MAAM,EAAEC,OAAO,GAAG,EAAE,EAAE;IAC/B,IAAI;MACFN,OAAO,CAACO,GAAG,CAAC,4BAA4B,EAAE,IAAI,CAACb,KAAK,CAAC;;MAErD;MACA,MAAMc,UAAU,GAAG,IAAIC,eAAe,CAAC,CAAC;MACxC,MAAMC,SAAS,GAAGC,UAAU,CAAC,MAAMH,UAAU,CAACI,KAAK,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC,CAAC;;MAE/D,MAAMhB,QAAQ,GAAG,MAAMN,KAAK,CAACuB,IAAI,CAAC,GAAG,IAAI,CAACpB,OAAO,eAAe,EAAE;QAChEC,KAAK,EAAE,IAAI,CAACA,KAAK;QACjBW,MAAM,EAAEA,MAAM,CAACS,SAAS,CAAC,CAAC,EAAE,IAAI,CAAC;QAAE;QACnCC,MAAM,EAAE,KAAK;QACbC,OAAO,EAAE;UACPC,WAAW,EAAE,GAAG;UAChBC,KAAK,EAAE,EAAE;UACTC,KAAK,EAAE,GAAG;UACVC,WAAW,EAAE,GAAG,CAAC;QACnB;MACF,CAAC,EAAE;QACDC,OAAO,EAAE;UACP,cAAc,EAAE;QAClB,CAAC;QACDC,MAAM,EAAEd,UAAU,CAACc,MAAM;QACzBC,OAAO,EAAE;MACX,CAAC,CAAC;MAEFC,YAAY,CAACd,SAAS,CAAC;MACvBV,OAAO,CAACO,GAAG,CAAC,0BAA0B,CAAC;MACvC,OAAOX,QAAQ,CAACM,IAAI,CAACN,QAAQ,IAAIA,QAAQ,CAACM,IAAI;IAChD,CAAC,CAAC,OAAOH,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,oBAAoB,EAAEA,KAAK,CAAC0B,OAAO,CAAC;MAElD,IAAI1B,KAAK,CAAC2B,IAAI,KAAK,cAAc,IAAI3B,KAAK,CAAC0B,OAAO,CAACE,QAAQ,CAAC,SAAS,CAAC,EAAE;QACtE,MAAM,IAAIC,KAAK,CAAC,wDAAwD,CAAC;MAC3E;MAEA,MAAM,IAAIA,KAAK,CAAC,qDAAqD,CAAC;IACxE;EACF;EAEA,MAAMC,UAAUA,CAACxB,MAAM,EAAEC,OAAO,GAAG,EAAE,EAAEwB,OAAO,EAAE;IAC9C,IAAI;MACF,MAAMlC,QAAQ,GAAG,MAAMmC,KAAK,CAAC,GAAG,IAAI,CAACtC,OAAO,eAAe,EAAE;QAC3DuC,MAAM,EAAE,MAAM;QACdX,OAAO,EAAE;UACP,cAAc,EAAE;QAClB,CAAC;QACDY,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;UACnBzC,KAAK,EAAE,IAAI,CAACA,KAAK;UACjBW,MAAM,EAAEA,MAAM;UACdC,OAAO,EAAEA,OAAO;UAChBS,MAAM,EAAE;QACV,CAAC;MACH,CAAC,CAAC;MAEF,MAAMqB,MAAM,GAAGxC,QAAQ,CAACqC,IAAI,CAACI,SAAS,CAAC,CAAC;MACxC,MAAMC,OAAO,GAAG,IAAIC,WAAW,CAAC,CAAC;MACjC,IAAIC,YAAY,GAAG,EAAE;MAErB,OAAO,IAAI,EAAE;QACX,MAAM;UAAEC,IAAI;UAAEC;QAAM,CAAC,GAAG,MAAMN,MAAM,CAACO,IAAI,CAAC,CAAC;QAC3C,IAAIF,IAAI,EAAE;QAEV,MAAMG,KAAK,GAAGN,OAAO,CAACO,MAAM,CAACH,KAAK,CAAC;QACnC,MAAMI,KAAK,GAAGF,KAAK,CAACG,KAAK,CAAC,IAAI,CAAC,CAACC,MAAM,CAACC,IAAI,IAAIA,IAAI,CAACC,IAAI,CAAC,CAAC,KAAK,EAAE,CAAC;QAElE,KAAK,MAAMD,IAAI,IAAIH,KAAK,EAAE;UACxB,IAAI;YACF,MAAMK,IAAI,GAAGjB,IAAI,CAACkB,KAAK,CAACH,IAAI,CAAC;YAC7B,IAAIE,IAAI,CAACvD,QAAQ,EAAE;cACjB4C,YAAY,IAAIW,IAAI,CAACvD,QAAQ;cAC7BkC,OAAO,CAACqB,IAAI,CAACvD,QAAQ,CAAC;YACxB;UACF,CAAC,CAAC,OAAOyD,CAAC,EAAE;YACVrD,OAAO,CAACD,KAAK,CAAC,wBAAwB,EAAEsD,CAAC,CAAC;UAC5C;QACF;MACF;MAEA,OAAOb,YAAY;IACrB,CAAC,CAAC,OAAOzC,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,2BAA2B,EAAEA,KAAK,CAAC;MACjD,MAAM,IAAI6B,KAAK,CAAC,uCAAuC,CAAC;IAC1D;EACF;EAEA0B,QAAQA,CAACC,SAAS,EAAE;IAClB,IAAI,CAAC7D,KAAK,GAAG6D,SAAS;EACxB;EAEA,MAAMC,sBAAsBA,CAACC,UAAU,EAAEC,cAAc,EAAE;IACvD,MAAMrD,MAAM,GAAG;AACnB;AACA;AACA,iBAAiBoD,UAAU,CAACE,KAAK;AACjC,yBAAyBD,cAAc,IAAI,OAAO;AAClD;AACA;AACA;AACA,KAAK;IAED,OAAO,MAAM,IAAI,CAACtD,IAAI,CAACC,MAAM,CAAC;EAChC;EAEA,MAAMuD,UAAUA,CAACC,GAAG,EAAE;IACpB,MAAMxD,MAAM,GAAG;AACnB;AACA;AACA,aAAa6B,IAAI,CAACC,SAAS,CAAC0B,GAAG,EAAE,IAAI,EAAE,CAAC,CAAC;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;IAED,OAAO,MAAM,IAAI,CAACzD,IAAI,CAACC,MAAM,CAAC;EAChC;AACF;AAEA,OAAO,MAAMyD,aAAa,GAAG,IAAIvE,aAAa,CAAC,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}