{"ast":null,"code":"import axios from 'axios';\nclass OllamaService {\n  constructor() {\n    this.baseURL = 'http://localhost:11434';\n    this.model = 'llama2'; // Default model, can be changed to any Ollama model\n  }\n  async checkConnection() {\n    try {\n      const response = await axios.get(`${this.baseURL}/api/tags`);\n      return response.status === 200;\n    } catch (error) {\n      console.error('Ollama connection check failed:', error);\n      return false;\n    }\n  }\n  async getModels() {\n    try {\n      const response = await axios.get(`${this.baseURL}/api/tags`);\n      return response.data.models || [];\n    } catch (error) {\n      console.error('Failed to get Ollama models:', error);\n      return [];\n    }\n  }\n  async chat(prompt, context = '') {\n    try {\n      const response = await axios.post(`${this.baseURL}/api/generate`, {\n        model: this.model,\n        prompt: prompt,\n        context: context,\n        stream: false,\n        options: {\n          temperature: 0.7,\n          top_p: 0.9,\n          max_tokens: 2000\n        }\n      });\n      return response.data.response;\n    } catch (error) {\n      console.error('Ollama chat error:', error);\n      throw new Error('Failed to get response from Ollama. Make sure Ollama is running.');\n    }\n  }\n  async streamChat(prompt, context = '', onChunk) {\n    try {\n      const response = await fetch(`${this.baseURL}/api/generate`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({\n          model: this.model,\n          prompt: prompt,\n          context: context,\n          stream: true\n        })\n      });\n      const reader = response.body.getReader();\n      const decoder = new TextDecoder();\n      let fullResponse = '';\n      while (true) {\n        const {\n          done,\n          value\n        } = await reader.read();\n        if (done) break;\n        const chunk = decoder.decode(value);\n        const lines = chunk.split('\\n').filter(line => line.trim() !== '');\n        for (const line of lines) {\n          try {\n            const json = JSON.parse(line);\n            if (json.response) {\n              fullResponse += json.response;\n              onChunk(json.response);\n            }\n          } catch (e) {\n            console.error('Failed to parse chunk:', e);\n          }\n        }\n      }\n      return fullResponse;\n    } catch (error) {\n      console.error('Ollama stream chat error:', error);\n      throw new Error('Failed to stream response from Ollama');\n    }\n  }\n  setModel(modelName) {\n    this.model = modelName;\n  }\n  async generatePRDSuggestions(prdSection, currentContent) {\n    const prompt = `\n      As a Product Manager, provide specific suggestions to improve this section of a Product Requirements Document.\n      \n      Section: ${prdSection.title}\n      Current Content: ${currentContent || 'Empty'}\n      \n      Provide 3-5 specific, actionable suggestions to improve this section.\n      Format your response as a numbered list.\n    `;\n    return await this.chat(prompt);\n  }\n  async analyzePRD(prd) {\n    const prompt = `\n      Analyze this Product Requirements Document and provide feedback.\n      \n      PRD: ${JSON.stringify(prd, null, 2)}\n      \n      Provide:\n      1. Completeness score (0-100%)\n      2. Missing critical sections\n      3. Areas that need more detail\n      4. Strengths of the current PRD\n      5. Top 3 recommendations for improvement\n      \n      Format your response in a clear, structured way.\n    `;\n    return await this.chat(prompt);\n  }\n}\nexport const ollamaService = new OllamaService();","map":{"version":3,"names":["axios","OllamaService","constructor","baseURL","model","checkConnection","response","get","status","error","console","getModels","data","models","chat","prompt","context","post","stream","options","temperature","top_p","max_tokens","Error","streamChat","onChunk","fetch","method","headers","body","JSON","stringify","reader","getReader","decoder","TextDecoder","fullResponse","done","value","read","chunk","decode","lines","split","filter","line","trim","json","parse","e","setModel","modelName","generatePRDSuggestions","prdSection","currentContent","title","analyzePRD","prd","ollamaService"],"sources":["/Volumes/t9/github/pmhelper/frontend/src/services/ollamaService.js"],"sourcesContent":["import axios from 'axios';\n\nclass OllamaService {\n  constructor() {\n    this.baseURL = 'http://localhost:11434';\n    this.model = 'llama2'; // Default model, can be changed to any Ollama model\n  }\n\n  async checkConnection() {\n    try {\n      const response = await axios.get(`${this.baseURL}/api/tags`);\n      return response.status === 200;\n    } catch (error) {\n      console.error('Ollama connection check failed:', error);\n      return false;\n    }\n  }\n\n  async getModels() {\n    try {\n      const response = await axios.get(`${this.baseURL}/api/tags`);\n      return response.data.models || [];\n    } catch (error) {\n      console.error('Failed to get Ollama models:', error);\n      return [];\n    }\n  }\n\n  async chat(prompt, context = '') {\n    try {\n      const response = await axios.post(`${this.baseURL}/api/generate`, {\n        model: this.model,\n        prompt: prompt,\n        context: context,\n        stream: false,\n        options: {\n          temperature: 0.7,\n          top_p: 0.9,\n          max_tokens: 2000\n        }\n      });\n\n      return response.data.response;\n    } catch (error) {\n      console.error('Ollama chat error:', error);\n      throw new Error('Failed to get response from Ollama. Make sure Ollama is running.');\n    }\n  }\n\n  async streamChat(prompt, context = '', onChunk) {\n    try {\n      const response = await fetch(`${this.baseURL}/api/generate`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({\n          model: this.model,\n          prompt: prompt,\n          context: context,\n          stream: true\n        })\n      });\n\n      const reader = response.body.getReader();\n      const decoder = new TextDecoder();\n      let fullResponse = '';\n\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        const chunk = decoder.decode(value);\n        const lines = chunk.split('\\n').filter(line => line.trim() !== '');\n        \n        for (const line of lines) {\n          try {\n            const json = JSON.parse(line);\n            if (json.response) {\n              fullResponse += json.response;\n              onChunk(json.response);\n            }\n          } catch (e) {\n            console.error('Failed to parse chunk:', e);\n          }\n        }\n      }\n\n      return fullResponse;\n    } catch (error) {\n      console.error('Ollama stream chat error:', error);\n      throw new Error('Failed to stream response from Ollama');\n    }\n  }\n\n  setModel(modelName) {\n    this.model = modelName;\n  }\n\n  async generatePRDSuggestions(prdSection, currentContent) {\n    const prompt = `\n      As a Product Manager, provide specific suggestions to improve this section of a Product Requirements Document.\n      \n      Section: ${prdSection.title}\n      Current Content: ${currentContent || 'Empty'}\n      \n      Provide 3-5 specific, actionable suggestions to improve this section.\n      Format your response as a numbered list.\n    `;\n\n    return await this.chat(prompt);\n  }\n\n  async analyzePRD(prd) {\n    const prompt = `\n      Analyze this Product Requirements Document and provide feedback.\n      \n      PRD: ${JSON.stringify(prd, null, 2)}\n      \n      Provide:\n      1. Completeness score (0-100%)\n      2. Missing critical sections\n      3. Areas that need more detail\n      4. Strengths of the current PRD\n      5. Top 3 recommendations for improvement\n      \n      Format your response in a clear, structured way.\n    `;\n\n    return await this.chat(prompt);\n  }\n}\n\nexport const ollamaService = new OllamaService();"],"mappings":"AAAA,OAAOA,KAAK,MAAM,OAAO;AAEzB,MAAMC,aAAa,CAAC;EAClBC,WAAWA,CAAA,EAAG;IACZ,IAAI,CAACC,OAAO,GAAG,wBAAwB;IACvC,IAAI,CAACC,KAAK,GAAG,QAAQ,CAAC,CAAC;EACzB;EAEA,MAAMC,eAAeA,CAAA,EAAG;IACtB,IAAI;MACF,MAAMC,QAAQ,GAAG,MAAMN,KAAK,CAACO,GAAG,CAAC,GAAG,IAAI,CAACJ,OAAO,WAAW,CAAC;MAC5D,OAAOG,QAAQ,CAACE,MAAM,KAAK,GAAG;IAChC,CAAC,CAAC,OAAOC,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,iCAAiC,EAAEA,KAAK,CAAC;MACvD,OAAO,KAAK;IACd;EACF;EAEA,MAAME,SAASA,CAAA,EAAG;IAChB,IAAI;MACF,MAAML,QAAQ,GAAG,MAAMN,KAAK,CAACO,GAAG,CAAC,GAAG,IAAI,CAACJ,OAAO,WAAW,CAAC;MAC5D,OAAOG,QAAQ,CAACM,IAAI,CAACC,MAAM,IAAI,EAAE;IACnC,CAAC,CAAC,OAAOJ,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,8BAA8B,EAAEA,KAAK,CAAC;MACpD,OAAO,EAAE;IACX;EACF;EAEA,MAAMK,IAAIA,CAACC,MAAM,EAAEC,OAAO,GAAG,EAAE,EAAE;IAC/B,IAAI;MACF,MAAMV,QAAQ,GAAG,MAAMN,KAAK,CAACiB,IAAI,CAAC,GAAG,IAAI,CAACd,OAAO,eAAe,EAAE;QAChEC,KAAK,EAAE,IAAI,CAACA,KAAK;QACjBW,MAAM,EAAEA,MAAM;QACdC,OAAO,EAAEA,OAAO;QAChBE,MAAM,EAAE,KAAK;QACbC,OAAO,EAAE;UACPC,WAAW,EAAE,GAAG;UAChBC,KAAK,EAAE,GAAG;UACVC,UAAU,EAAE;QACd;MACF,CAAC,CAAC;MAEF,OAAOhB,QAAQ,CAACM,IAAI,CAACN,QAAQ;IAC/B,CAAC,CAAC,OAAOG,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,oBAAoB,EAAEA,KAAK,CAAC;MAC1C,MAAM,IAAIc,KAAK,CAAC,kEAAkE,CAAC;IACrF;EACF;EAEA,MAAMC,UAAUA,CAACT,MAAM,EAAEC,OAAO,GAAG,EAAE,EAAES,OAAO,EAAE;IAC9C,IAAI;MACF,MAAMnB,QAAQ,GAAG,MAAMoB,KAAK,CAAC,GAAG,IAAI,CAACvB,OAAO,eAAe,EAAE;QAC3DwB,MAAM,EAAE,MAAM;QACdC,OAAO,EAAE;UACP,cAAc,EAAE;QAClB,CAAC;QACDC,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;UACnB3B,KAAK,EAAE,IAAI,CAACA,KAAK;UACjBW,MAAM,EAAEA,MAAM;UACdC,OAAO,EAAEA,OAAO;UAChBE,MAAM,EAAE;QACV,CAAC;MACH,CAAC,CAAC;MAEF,MAAMc,MAAM,GAAG1B,QAAQ,CAACuB,IAAI,CAACI,SAAS,CAAC,CAAC;MACxC,MAAMC,OAAO,GAAG,IAAIC,WAAW,CAAC,CAAC;MACjC,IAAIC,YAAY,GAAG,EAAE;MAErB,OAAO,IAAI,EAAE;QACX,MAAM;UAAEC,IAAI;UAAEC;QAAM,CAAC,GAAG,MAAMN,MAAM,CAACO,IAAI,CAAC,CAAC;QAC3C,IAAIF,IAAI,EAAE;QAEV,MAAMG,KAAK,GAAGN,OAAO,CAACO,MAAM,CAACH,KAAK,CAAC;QACnC,MAAMI,KAAK,GAAGF,KAAK,CAACG,KAAK,CAAC,IAAI,CAAC,CAACC,MAAM,CAACC,IAAI,IAAIA,IAAI,CAACC,IAAI,CAAC,CAAC,KAAK,EAAE,CAAC;QAElE,KAAK,MAAMD,IAAI,IAAIH,KAAK,EAAE;UACxB,IAAI;YACF,MAAMK,IAAI,GAAGjB,IAAI,CAACkB,KAAK,CAACH,IAAI,CAAC;YAC7B,IAAIE,IAAI,CAACzC,QAAQ,EAAE;cACjB8B,YAAY,IAAIW,IAAI,CAACzC,QAAQ;cAC7BmB,OAAO,CAACsB,IAAI,CAACzC,QAAQ,CAAC;YACxB;UACF,CAAC,CAAC,OAAO2C,CAAC,EAAE;YACVvC,OAAO,CAACD,KAAK,CAAC,wBAAwB,EAAEwC,CAAC,CAAC;UAC5C;QACF;MACF;MAEA,OAAOb,YAAY;IACrB,CAAC,CAAC,OAAO3B,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,2BAA2B,EAAEA,KAAK,CAAC;MACjD,MAAM,IAAIc,KAAK,CAAC,uCAAuC,CAAC;IAC1D;EACF;EAEA2B,QAAQA,CAACC,SAAS,EAAE;IAClB,IAAI,CAAC/C,KAAK,GAAG+C,SAAS;EACxB;EAEA,MAAMC,sBAAsBA,CAACC,UAAU,EAAEC,cAAc,EAAE;IACvD,MAAMvC,MAAM,GAAG;AACnB;AACA;AACA,iBAAiBsC,UAAU,CAACE,KAAK;AACjC,yBAAyBD,cAAc,IAAI,OAAO;AAClD;AACA;AACA;AACA,KAAK;IAED,OAAO,MAAM,IAAI,CAACxC,IAAI,CAACC,MAAM,CAAC;EAChC;EAEA,MAAMyC,UAAUA,CAACC,GAAG,EAAE;IACpB,MAAM1C,MAAM,GAAG;AACnB;AACA;AACA,aAAae,IAAI,CAACC,SAAS,CAAC0B,GAAG,EAAE,IAAI,EAAE,CAAC,CAAC;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;IAED,OAAO,MAAM,IAAI,CAAC3C,IAAI,CAACC,MAAM,CAAC;EAChC;AACF;AAEA,OAAO,MAAM2C,aAAa,GAAG,IAAIzD,aAAa,CAAC,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}